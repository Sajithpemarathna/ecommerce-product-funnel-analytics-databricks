{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df4002e9-e426-4cf1-8d76-020739efcfb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# SILVER LAYER CONFIG\n",
    "# =========================================================\n",
    "SOURCE_TABLE = \"workspace.default.bronze_events\"\n",
    "SILVER_TABLE = \"workspace.default.silver_events\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d02981b-f4dc-4ef4-8820-ceb281255689",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "727c4ce5-d7e6-48f6-b48e-7ab17dd681a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze = spark.table(SOURCE_TABLE)\n",
    "\n",
    "print(\"Bronze rows:\", df_bronze.count())\n",
    "print(\"Bronze columns:\", df_bronze.columns)\n",
    "\n",
    "display(df_bronze.limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa8b3de9-e7c0-436d-ab73-d2a558b66b41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create an empty silver table with expected columns if it doesn't exist\n",
    "if not spark.catalog.tableExists(SILVER_TABLE):\n",
    "    df_empty = (\n",
    "        df_bronze.limit(0)\n",
    "        .withColumn(\"event_time\", F.to_timestamp(\"event_time\"))\n",
    "        .withColumn(\"event_date\", F.to_date(\"event_time\"))\n",
    "        .withColumn(\"event_key\", F.lit(None).cast(\"string\"))\n",
    "    )\n",
    "\n",
    "    (df_empty.write.format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .saveAsTable(SILVER_TABLE))\n",
    "\n",
    "    print(f\"✅ Created Silver table: {SILVER_TABLE}\")\n",
    "else:\n",
    "    print(f\"✅ Silver table already exists: {SILVER_TABLE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d833d43-b451-4342-b829-5273930964c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver_existing = spark.table(SILVER_TABLE)\n",
    "\n",
    "max_silver_time = df_silver_existing.select(F.max(\"event_time\").alias(\"max_t\")).collect()[0][\"max_t\"]\n",
    "print(\"Max event_time in Silver:\", max_silver_time)\n",
    "\n",
    "df_bronze_typed = (\n",
    "    df_bronze\n",
    "    .withColumn(\"event_time\", F.to_timestamp(\"event_time\"))\n",
    "    .withColumn(\"event_date\", F.to_date(\"event_time\"))\n",
    ")\n",
    "\n",
    "if max_silver_time is None:\n",
    "    df_new = df_bronze_typed\n",
    "else:\n",
    "    df_new = df_bronze_typed.filter(F.col(\"event_time\") > F.lit(max_silver_time))\n",
    "\n",
    "print(\"New Bronze rows to process:\", df_new.count())\n",
    "display(df_new.orderBy(F.col(\"event_time\")).limit(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "175bd208-c927-4f99-900e-a25d8cb4a322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols = df_new.columns\n",
    "\n",
    "df_clean = df_new\n",
    "\n",
    "# --- Standard cleaning rules (only applied if column exists) ---\n",
    "if \"event_type\" in cols:\n",
    "    df_clean = df_clean.withColumn(\"event_type\", F.lower(F.trim(F.col(\"event_type\"))))\n",
    "\n",
    "if \"user_id\" in cols:\n",
    "    df_clean = df_clean.withColumn(\"user_id\", F.col(\"user_id\").cast(\"long\"))\n",
    "\n",
    "if \"product_id\" in cols:\n",
    "    df_clean = df_clean.withColumn(\"product_id\", F.col(\"product_id\").cast(\"long\"))\n",
    "\n",
    "if \"price\" in cols:\n",
    "    df_clean = df_clean.withColumn(\"price\", F.col(\"price\").cast(\"double\"))\n",
    "\n",
    "# Drop rows with no event_time (invalid)\n",
    "df_clean = df_clean.filter(F.col(\"event_time\").isNotNull())\n",
    "\n",
    "display(df_clean.limit(10))\n",
    "\n",
    "if \"brand\" in cols:\n",
    "    df_clean = df_clean.withColumn(\n",
    "        \"brand\",\n",
    "        F.when(F.col(\"brand\").isNull() | (F.trim(F.col(\"brand\")) == \"\"), F.lit(\"unknown\"))\n",
    "         .otherwise(F.lower(F.trim(F.col(\"brand\"))))\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb7a92d7-a849-4126-b3e1-7376e720381b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# choose columns to build a stable key (use only existing ones)\n",
    "key_candidates = [\"event_time\", \"event_type\", \"user_id\", \"product_id\", \"session_id\", \"order_id\"]\n",
    "key_cols = [c for c in key_candidates if c in df_clean.columns]\n",
    "\n",
    "if len(key_cols) == 0:\n",
    "    raise ValueError(\"No suitable columns found to build event_key. Check bronze schema.\")\n",
    "\n",
    "df_keyed = df_clean.withColumn(\n",
    "    \"event_key\",\n",
    "    F.sha2(F.concat_ws(\"||\", *[F.col(c).cast(\"string\") for c in key_cols]), 256)\n",
    ")\n",
    "\n",
    "# Deduplicate within the incoming batch\n",
    "w = Window.partitionBy(\"event_key\").orderBy(F.col(\"event_time\").desc())\n",
    "df_dedup = (\n",
    "    df_keyed\n",
    "    .withColumn(\"rn\", F.row_number().over(w))\n",
    "    .filter(F.col(\"rn\") == 1)\n",
    "    .drop(\"rn\")\n",
    ")\n",
    "\n",
    "print(\"After dedup (incoming batch):\", df_dedup.count())\n",
    "display(df_dedup.limit(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a9af35a-6ed5-4aab-b0ff-ca26232e9e5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SILVER_TABLE = \"workspace.default.silver_events\"\n",
    "\n",
    "# 1) Check current silver schema\n",
    "df_t = spark.table(SILVER_TABLE)\n",
    "print(\"Silver columns:\", df_t.columns)\n",
    "\n",
    "# 2) Add missing columns safely (only if not exists)\n",
    "missing = []\n",
    "if \"event_date\" not in df_t.columns:\n",
    "    missing.append(\"event_date DATE\")\n",
    "if \"event_key\" not in df_t.columns:\n",
    "    missing.append(\"event_key STRING\")\n",
    "\n",
    "if missing:\n",
    "    spark.sql(f\"ALTER TABLE {SILVER_TABLE} ADD COLUMNS ({', '.join(missing)})\")\n",
    "    print(\"✅ Added missing columns:\", missing)\n",
    "else:\n",
    "    print(\"✅ Silver already has event_date + event_key\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1923d4ae-75eb-45b4-b56a-1bf0d26ca0d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "silver_dt = DeltaTable.forName(spark, SILVER_TABLE)\n",
    "\n",
    "# ✅ If there is no new data, skip merge (prevents unnecessary job failures)\n",
    "if df_dedup.count() == 0:\n",
    "    print(\"✅ No new rows to MERGE into Silver (incoming batch is empty). Skipping.\")\n",
    "else:\n",
    "    all_cols = df_dedup.columns\n",
    "    update_map = {c: f\"s.{c}\" for c in all_cols}\n",
    "    insert_map = {c: f\"s.{c}\" for c in all_cols}\n",
    "\n",
    "    (\n",
    "        silver_dt.alias(\"t\")\n",
    "        .merge(\n",
    "            df_dedup.alias(\"s\"),\n",
    "            \"t.event_key = s.event_key\"\n",
    "        )\n",
    "        .whenMatchedUpdate(set=update_map)\n",
    "        .whenNotMatchedInsert(values=insert_map)\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "    print(\"✅ Silver MERGE completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db1dc995-f818-4842-bb85-3e0b906bbd01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(spark.table(SILVER_TABLE).columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7718f9b8-00df-4966-9c80-3cf507e20cb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze = spark.table(\"workspace.default.bronze_events\")\n",
    "display(bronze.selectExpr(\"max(to_timestamp(event_time)) as max_bronze_time\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6c444d4-2e60-4260-9cc1-8af9975d1228",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver = spark.table(\"workspace.default.silver_events\")\n",
    "display(silver.selectExpr(\"max(event_time) as max_silver_time\"))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_silver_events_cleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
