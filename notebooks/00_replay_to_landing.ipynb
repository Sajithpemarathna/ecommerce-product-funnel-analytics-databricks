{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7282ae16-28c0-49d9-a344-86630753d566",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Source raw delta table you created\n",
    "RAW_TABLE = \"workspace.default.raw_events_2019_dec\"\n",
    "\n",
    "# State table to remember progress\n",
    "STATE_TABLE = \"monitoring.pipeline_state\"\n",
    "\n",
    "PIPELINE_NAME = \"ecomm_events_daily_replay\"\n",
    "\n",
    "# Landing zone (new daily slice will be written here each run)\n",
    "LANDING_BASE = \"/Volumes/workspace/default/landing/events\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b73d5c0-f863-4719-a522-10688573d71f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS monitoring\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {STATE_TABLE} (\n",
    "  pipeline_name STRING,\n",
    "  last_released_date DATE,\n",
    "  updated_at TIMESTAMP\n",
    ")\n",
    "USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# Ensure one row exists for this pipeline\n",
    "spark.sql(f\"\"\"\n",
    "MERGE INTO {STATE_TABLE} t\n",
    "USING (SELECT '{PIPELINE_NAME}' AS pipeline_name) s\n",
    "ON t.pipeline_name = s.pipeline_name\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (pipeline_name, last_released_date, updated_at)\n",
    "  VALUES (s.pipeline_name, DATE('2019-11-30'), current_timestamp())\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbdd1873-110c-48d4-919c-0952a36d734c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "state = (spark.table(STATE_TABLE)\n",
    "         .filter(F.col(\"pipeline_name\") == PIPELINE_NAME)\n",
    "         .select(\"last_released_date\")\n",
    "         .first())\n",
    "\n",
    "last_released = state[\"last_released_date\"]\n",
    "next_date = spark.sql(f\"SELECT date_add(DATE('{last_released}'), 1) AS d\").first()[\"d\"]\n",
    "\n",
    "print(\"last_released_date:\", last_released)\n",
    "print(\"next_date_to_release:\", next_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5805e693-9ea4-4520-8cf4-e5eae12cb5ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw = spark.table(RAW_TABLE)\n",
    "\n",
    "exists = raw.filter(F.col(\"event_date\") == F.lit(next_date)).limit(1).count() > 0\n",
    "print(\"date exists in raw table?\", exists)\n",
    "\n",
    "if not exists:\n",
    "    print(\"✅ No more dates left to release. Pipeline is finished for December.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "157ef38e-83c3-448a-a62b-18595e241bd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS workspace.default.landing;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a44ba9bf-7737-4ff5-bb43-227cb4d3264d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if exists:\n",
    "    day_df = raw.filter(F.col(\"event_date\") == F.lit(next_date)) \\\n",
    "                .withColumn(\"replay_ts\", F.current_timestamp())\n",
    "\n",
    "    target_path = f\"{LANDING_BASE}/event_date={next_date}\"\n",
    "\n",
    "    # Idempotent for that day: overwrite only that day's landing folder\n",
    "    (day_df.write\n",
    "         .mode(\"overwrite\")\n",
    "         .parquet(target_path))\n",
    "\n",
    "    released_rows = day_df.count()\n",
    "    print(f\"✅ Released {released_rows} rows to: {target_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e0d0ef8-f7bc-4ad9-a664-2159c9538a74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if exists:\n",
    "    spark.sql(f\"\"\"\n",
    "      UPDATE {STATE_TABLE}\n",
    "      SET last_released_date = DATE('{next_date}'),\n",
    "          updated_at = current_timestamp()\n",
    "      WHERE pipeline_name = '{PIPELINE_NAME}'\n",
    "    \"\"\")\n",
    "    print(\"✅ State updated. Next run will release the next day.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "369b29b6-17b8-4dbb-9e8a-9b79aa903eba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if exists:\n",
    "    sample = (spark.read\n",
    "              .parquet(f\"{LANDING_BASE}/event_date={next_date}\")\n",
    "              .count())\n",
    "    print(\"Landing rows (read back):\", sample)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6814837506788770,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "00_replay_to_landing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
