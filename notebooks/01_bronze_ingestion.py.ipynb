{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a835229b-53c6-455e-aa8a-c684e8acc2f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze Ingestion – Raw E-commerce Events\n",
    "\n",
    "This notebook ingests raw e-commerce event data into the Bronze layer.\n",
    "No transformations are applied at this stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfbdebf6-e843-4ad3-b30c-caac0ddd8bac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Table names for source and bronze target\n",
    "SOURCE_TABLE = \"workspace.default.events\"\n",
    "BRONZE_TABLE = \"workspace.default.bronze_events\"\n",
    "\n",
    "# Small state table to remember the last processed date\n",
    "STATE_TABLE = \"workspace.default.bronze_ingestion_state\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae93b572-5434-43a6-81a9-106aec6db927",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Spark functions + Python date helpers\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c1e60f5-1985-44d7-a913-cae7e9c83540",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load source events and derive event_date for daily chunking\n",
    "df_src = spark.table(SOURCE_TABLE).withColumn(\"event_date\", F.to_date(\"event_time\"))\n",
    "\n",
    "# Quick date range check (used to stop when no more new days exist)\n",
    "min_date = df_src.select(F.min(\"event_date\").alias(\"min_date\")).collect()[0][\"min_date\"]\n",
    "max_date = df_src.select(F.max(\"event_date\").alias(\"max_date\")).collect()[0][\"max_date\"]\n",
    "\n",
    "print(\"Min event_date:\", min_date)\n",
    "print(\"Max event_date:\", max_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46a424e7-7d52-46d8-8e67-3c04305cb212",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the state table once (stores last processed day)\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {STATE_TABLE} (\n",
    "  last_processed_date DATE\n",
    ")\n",
    "USING delta\n",
    "\"\"\")\n",
    "\n",
    "# If state is empty, initialize last_processed_date = (min_date - 1 day)\n",
    "state_count = spark.table(STATE_TABLE).count()\n",
    "\n",
    "if state_count == 0:\n",
    "    init_last = min_date - timedelta(days=1)\n",
    "    spark.createDataFrame([(init_last,)], [\"last_processed_date\"]).write.format(\"delta\").mode(\"overwrite\").saveAsTable(STATE_TABLE)\n",
    "\n",
    "# Read last processed date as a Python date\n",
    "last_processed = spark.table(STATE_TABLE).select(\"last_processed_date\").collect()[0][\"last_processed_date\"]\n",
    "print(\"Last processed:\", last_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5dda459-b0da-45e2-9c92-dff910fe3c38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Process one day per run (job-friendly incremental)\n",
    "next_date = last_processed + timedelta(days=1)\n",
    "print(\"Next date:\", next_date, \"| Max date:\", max_date)\n",
    "\n",
    "# If nothing new left, exit cleanly (job will show as succeeded)\n",
    "if next_date > max_date:\n",
    "    dbutils.notebook.exit(\"No new dates to process.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7cdac4b-801d-4771-a8cd-f81cb3fa75eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter source to one day only (the \"chunk\")\n",
    "df_day = df_src.filter(F.col(\"event_date\") == F.lit(next_date))\n",
    "\n",
    "day_count = df_day.count()\n",
    "print(\"Rows for next_date:\", day_count)\n",
    "\n",
    "# If day has no rows (rare), still advance the state and exit\n",
    "if day_count == 0:\n",
    "    spark.createDataFrame([(next_date,)], [\"last_processed_date\"]).write.format(\"delta\").mode(\"overwrite\").saveAsTable(STATE_TABLE)\n",
    "    dbutils.notebook.exit(f\"No rows for {next_date}, state advanced.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d81f186d-9611-4b93-a24b-c71d9252b04e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Bronze table if missing (schema inferred from first write)\n",
    "# Write only this day and keep reruns safe using replaceWhere\n",
    "(\n",
    "    df_day.drop(\"event_date\")  # keep bronze raw schema same as before (optional)\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"replaceWhere\", f\"to_date(event_time) = '{next_date}'\")\n",
    "    .saveAsTable(BRONZE_TABLE)\n",
    ")\n",
    "\n",
    "print(f\"✅ Bronze written for date: {next_date} into {BRONZE_TABLE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cefb7b9-3de1-4c72-8226-956beb8da318",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Update state to mark this day as completed\n",
    "spark.createDataFrame([(next_date,)], [\"last_processed_date\"]).write.format(\"delta\").mode(\"overwrite\").saveAsTable(STATE_TABLE)\n",
    "\n",
    "print(\"✅ State updated. Last processed is now:\", next_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5e83fab-3f9d-4001-a0c0-8129d0a0b000",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Quick check: confirm bronze has rows for that date\n",
    "bronze_day_rows = spark.table(BRONZE_TABLE).filter(F.to_date(\"event_time\") == F.lit(next_date)).count()\n",
    "print(\"Bronze rows for processed date:\", bronze_day_rows)\n",
    "\n",
    "display(spark.table(BRONZE_TABLE).orderBy(F.col(\"event_time\")).limit(20))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_bronze_ingestion.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
